# PIPELINE DEFINITION
# Name: diabetes-classification-pipeline
# Description: A demonstration pipeline for diabetes classification using multiple models with artifact tracking
# Inputs:
#    dt_max_depth: int [Default: 5.0]
#    random_state: int [Default: 42.0]
#    rf_n_estimators: int [Default: 100.0]
#    svm_c: float [Default: 1.0]
#    svm_kernel: str [Default: 'rbf']
#    test_size: float [Default: 0.3]
components:
  comp-compare-models:
    executorLabel: exec-compare-models
    inputDefinitions:
      artifacts:
        dt_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        rf_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        svm_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        best_model_info:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        comparison_result:
          artifactType:
            schemaTitle: system.HTML
            schemaVersion: 0.0.1
  comp-evaluate-model:
    executorLabel: exec-evaluate-model
    inputDefinitions:
      artifacts:
        feature_names:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        test_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        model_name:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        confusion_matrix:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
        evaluation_plots:
          artifactType:
            schemaTitle: system.HTML
            schemaVersion: 0.0.1
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
  comp-evaluate-model-2:
    executorLabel: exec-evaluate-model-2
    inputDefinitions:
      artifacts:
        feature_names:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        test_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        model_name:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        confusion_matrix:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
        evaluation_plots:
          artifactType:
            schemaTitle: system.HTML
            schemaVersion: 0.0.1
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
  comp-evaluate-model-3:
    executorLabel: exec-evaluate-model-3
    inputDefinitions:
      artifacts:
        feature_names:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        test_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        model_name:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        confusion_matrix:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
        evaluation_plots:
          artifactType:
            schemaTitle: system.HTML
            schemaVersion: 0.0.1
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
  comp-load-data:
    executorLabel: exec-load-data
    outputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        feature_names:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        metadata:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-preprocess-data:
    executorLabel: exec-preprocess-data
    inputDefinitions:
      artifacts:
        test_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        preprocessor:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        processed_test_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        processed_train_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-split-data:
    executorLabel: exec-split-data
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        random_state:
          defaultValue: 42.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        test_size:
          defaultValue: 0.3
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        split_info:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        test_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train-decision-tree:
    executorLabel: exec-train-decision-tree
    inputDefinitions:
      artifacts:
        train_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        max_depth:
          defaultValue: 5.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        random_state:
          defaultValue: 42.0
          isOptional: true
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-train-random-forest:
    executorLabel: exec-train-random-forest
    inputDefinitions:
      artifacts:
        train_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        n_estimators:
          defaultValue: 100.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        random_state:
          defaultValue: 42.0
          isOptional: true
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-train-svm:
    executorLabel: exec-train-svm
    inputDefinitions:
      artifacts:
        train_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        C:
          defaultValue: 1.0
          isOptional: true
          parameterType: NUMBER_DOUBLE
        kernel:
          defaultValue: rbf
          isOptional: true
          parameterType: STRING
        random_state:
          defaultValue: 42.0
          isOptional: true
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-compare-models:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - compare_models
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'matplotlib'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef compare_models(dt_metrics: Input[Metrics], \n               \
          \   rf_metrics: Input[Metrics], \n                  svm_metrics: Input[Metrics],\n\
          \                  comparison_result: Output[HTML],\n                  best_model_info:\
          \ Output[Metrics]):\n    import json\n    import matplotlib.pyplot as plt\n\
          \    import base64\n    import datetime\n    from io import BytesIO\n\n\
          \    # Extract metrics\n    model_names = [\"Decision Tree\", \"Random Forest\"\
          , \"Support Vector Machine\"]\n    metrics_artifacts = [dt_metrics, rf_metrics,\
          \ svm_metrics]\n\n    # Collect metrics\n    accuracies = [metrics.metadata[\"\
          accuracy\"] for metrics in metrics_artifacts]\n    if \"roc_auc\" in dt_metrics.metadata:\n\
          \        aucs = [metrics.metadata.get(\"roc_auc\", 0) for metrics in metrics_artifacts]\n\
          \    else:\n        aucs = None\n\n    # Find the best model based on accuracy\n\
          \    best_model_idx = accuracies.index(max(accuracies))\n    best_model\
          \ = model_names[best_model_idx]\n\n    # Create comparison plot for accuracy\n\
          \    plt.figure(figsize=(10, 6))\n    bars = plt.bar(model_names, accuracies,\
          \ color=['blue', 'green', 'red'])\n    plt.xlabel('Model')\n    plt.ylabel('Accuracy')\n\
          \    plt.title('Model Comparison - Diabetes Classification')\n    plt.ylim([0,\
          \ 1])\n\n    # Highlight the best model\n    bars[best_model_idx].set_color('gold')\n\
          \n    # Add text annotations\n    for i, acc in enumerate(accuracies):\n\
          \        plt.text(i, acc + 0.02, f'{acc:.4f}', ha='center')\n\n    plt.tight_layout()\n\
          \n    # Save the comparison plot\n    buffer = BytesIO()\n    plt.savefig(buffer,\
          \ format='png')\n    buffer.seek(0)\n    accuracy_comparison_image = base64.b64encode(buffer.read()).decode('utf-8')\n\
          \    plt.close()\n\n    # Create AUC comparison if available\n    auc_comparison_image\
          \ = None\n    if aucs:\n        plt.figure(figsize=(10, 6))\n        bars\
          \ = plt.bar(model_names, aucs, color=['blue', 'green', 'red'])\n       \
          \ plt.xlabel('Model')\n        plt.ylabel('ROC AUC')\n        plt.title('ROC\
          \ AUC Comparison - Diabetes Classification')\n        plt.ylim([0, 1])\n\
          \n        # Highlight the best model by AUC\n        best_auc_idx = aucs.index(max(aucs))\n\
          \        bars[best_auc_idx].set_color('gold')\n\n        # Add text annotations\n\
          \        for i, auc_val in enumerate(aucs):\n            plt.text(i, auc_val\
          \ + 0.02, f'{auc_val:.4f}', ha='center')\n\n        plt.tight_layout()\n\
          \n        # Save the AUC comparison plot\n        buffer = BytesIO()\n \
          \       plt.savefig(buffer, format='png')\n        buffer.seek(0)\n    \
          \    auc_comparison_image = base64.b64encode(buffer.read()).decode('utf-8')\n\
          \        plt.close()\n\n    # Create HTML for visual comparison\n    html_content\
          \ = \"\"\"\n    <html>\n    <head>\n        <title>Model Comparison Results</title>\n\
          \        <style>\n            body { font-family: Arial, sans-serif; margin:\
          \ 20px; }\n            .comparison-container { margin: 20px 0; }\n     \
          \       .winner { \n                background-color: #fcf8e3; \n      \
          \          border: 1px solid #faebcc; \n                padding: 15px; \n\
          \                border-radius: 5px;\n                margin: 20px 0;\n\
          \            }\n            table { \n                border-collapse: collapse;\
          \ \n                width: 100%; \n                margin: 20px 0;\n   \
          \         }\n            th, td { \n                border: 1px solid #ddd;\
          \ \n                padding: 8px; \n                text-align: center;\
          \ \n            }\n            th { \n                background-color:\
          \ #f2f2f2; \n                font-weight: bold; \n            }\n      \
          \      .best { \n                font-weight: bold; \n                color:\
          \ #5cb85c;\n            }\n        </style>\n    </head>\n    <body>\n \
          \       <h1>Model Comparison Results</h1>\n\n        <div class=\"winner\"\
          >\n            <h2>Best Model: \"\"\" + best_model + \"\"\"</h2>\n     \
          \       <p>Accuracy: \"\"\" + f\"{accuracies[best_model_idx]:.4f}\" + \"\
          \"\"</p>\n    \"\"\"\n\n    if aucs:\n        html_content += \"<p>ROC AUC:\
          \ \" + f\"{aucs[best_model_idx]:.4f}\" + \"</p>\"\n\n    html_content +=\
          \ \"\"\"\n        </div>\n\n        <h2>Performance Metrics Comparison</h2>\n\
          \        <table>\n            <tr>\n                <th>Model</th>\n   \
          \             <th>Accuracy</th>\n    \"\"\"\n\n    if aucs:\n        html_content\
          \ += \"<th>ROC AUC</th>\"\n\n    html_content += \"</tr>\"\n\n    # Add\
          \ rows for each model\n    for i, model in enumerate(model_names):\n   \
          \     is_best = (i == best_model_idx)\n        html_content += f\"\"\"\n\
          \        <tr>\n            <td>{\"<strong>\" if is_best else \"\"}{model}{\"\
          </strong>\" if is_best else \"\"}</td>\n            <td class=\"{'best'\
          \ if is_best else ''}\">{accuracies[i]:.4f}</td>\n        \"\"\"\n\n   \
          \     if aucs:\n            is_best_auc = (i == aucs.index(max(aucs)))\n\
          \            html_content += f'<td class=\"{\"best\" if is_best_auc else\
          \ \"\"}\">{aucs[i]:.4f}</td>'\n\n        html_content += \"</tr>\"\n\n \
          \   html_content += \"\"\"\n        </table>\n\n        <div class=\"comparison-container\"\
          >\n            <h2>Accuracy Comparison</h2>\n            <img src=\"data:image/png;base64,\"\
          \"\" + accuracy_comparison_image + \"\"\"\" alt=\"Accuracy Comparison\"\
          >\n        </div>\n    \"\"\"\n\n    if auc_comparison_image:\n        html_content\
          \ += \"\"\"\n        <div class=\"comparison-container\">\n            <h2>ROC\
          \ AUC Comparison</h2>\n            <img src=\"data:image/png;base64,\"\"\
          \" + auc_comparison_image + \"\"\"\" alt=\"AUC Comparison\">\n        </div>\n\
          \        \"\"\"\n\n    html_content += \"\"\"\n    </body>\n    </html>\n\
          \    \"\"\"\n\n    # Write HTML to output\n    with open(comparison_result.path,\
          \ 'w') as f:\n        f.write(html_content)\n\n    # Save best model info\n\
          \    best_model_info.metadata.update({\n        'best_model': best_model,\n\
          \        'accuracy': accuracies[best_model_idx],\n        'creation_time':\
          \ str(datetime.datetime.now()),\n    })\n\n    if aucs:\n        best_model_info.metadata['roc_auc']\
          \ = aucs[best_model_idx]\n\n    # Add comparison data to metadata\n    for\
          \ i, model_name in enumerate(model_names):\n        best_model_info.metadata[f\"\
          {model_name.lower().replace(' ', '_')}_accuracy\"] = accuracies[i]\n   \
          \     if aucs:\n            best_model_info.metadata[f\"{model_name.lower().replace('\
          \ ', '_')}_auc\"] = aucs[i]\n\n"
        image: python:3.9
    exec-evaluate-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - evaluate_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'numpy' 'scikit-learn'\
          \ 'joblib' 'matplotlib' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef evaluate_model(model: Input[Model], \n                  test_data:\
          \ Input[Dataset], \n                  feature_names: Input[Dataset], \n\
          \                  model_name: str,\n                  metrics: Output[Metrics],\n\
          \                  confusion_matrix: Output[ClassificationMetrics],\n  \
          \                evaluation_plots: Output[HTML]):\n    import joblib\n \
          \   import numpy as np\n    import json\n    import os\n    import datetime\n\
          \    from sklearn.metrics import accuracy_score, classification_report,\
          \ confusion_matrix as cm\n    from sklearn.metrics import roc_curve, auc,\
          \ precision_recall_curve\n    import matplotlib.pyplot as plt\n    import\
          \ base64\n    from io import BytesIO\n\n    # Load model, test data, and\
          \ feature names\n    model_obj = joblib.load(model.path)\n    test_data_combined\
          \ = joblib.load(test_data.path)\n    feature_names_list = joblib.load(feature_names.path)\n\
          \n    # Extract features and target\n    X_test = test_data_combined[:,\
          \ :-1]\n    y_test = test_data_combined[:, -1]\n\n    # Make predictions\n\
          \    y_pred = model_obj.predict(X_test)\n    y_pred_proba = model_obj.predict_proba(X_test)[:,\
          \ 1] if hasattr(model_obj, \"predict_proba\") else None\n\n    # Calculate\
          \ metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    class_report\
          \ = classification_report(y_test, y_pred, output_dict=True)\n    conf_matrix\
          \ = cm(y_test, y_pred).tolist()\n\n    # Print the keys of class_report\
          \ to help debugging\n    print(\"Classification report keys:\", class_report.keys())\n\
          \n    # Store metrics - check if keys are strings or integers\n    metrics_dict\
          \ = {\n        \"accuracy\": float(accuracy)\n    }\n\n    # Handle different\
          \ possible key formats in classification report\n    if '0' in class_report:\n\
          \        # String keys\n        metrics_dict.update({\n            \"precision_class_0\"\
          : class_report['0']['precision'],\n            \"recall_class_0\": class_report['0']['recall'],\n\
          \            \"f1_score_class_0\": class_report['0']['f1-score'],\n    \
          \        \"precision_class_1\": class_report['1']['precision'],\n      \
          \      \"recall_class_1\": class_report['1']['recall'],\n            \"\
          f1_score_class_1\": class_report['1']['f1-score'],\n        })\n    elif\
          \ 0 in class_report:\n        # Integer keys\n        metrics_dict.update({\n\
          \            \"precision_class_0\": class_report[0]['precision'],\n    \
          \        \"recall_class_0\": class_report[0]['recall'],\n            \"\
          f1_score_class_0\": class_report[0]['f1-score'],\n            \"precision_class_1\"\
          : class_report[1]['precision'],\n            \"recall_class_1\": class_report[1]['recall'],\n\
          \            \"f1_score_class_1\": class_report[1]['f1-score'],\n      \
          \  })\n    else:\n        # If neither format works, try to find the actual\
          \ keys\n        # This handles cases where scikit-learn might label classes\
          \ differently\n        class_keys = [k for k in class_report.keys() if k\
          \ not in ['accuracy', 'macro avg', 'weighted avg']]\n        if len(class_keys)\
          \ >= 2:\n            metrics_dict.update({\n                \"precision_class_0\"\
          : class_report[class_keys[0]]['precision'],\n                \"recall_class_0\"\
          : class_report[class_keys[0]]['recall'],\n                \"f1_score_class_0\"\
          : class_report[class_keys[0]]['f1-score'],\n                \"precision_class_1\"\
          : class_report[class_keys[1]]['precision'],\n                \"recall_class_1\"\
          : class_report[class_keys[1]]['recall'],\n                \"f1_score_class_1\"\
          : class_report[class_keys[1]]['f1-score'],\n            })\n\n    # Calculate\
          \ ROC and AUC if probabilities are available\n    if y_pred_proba is not\
          \ None:\n        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n       \
          \ roc_auc = auc(fpr, tpr)\n        metrics_dict[\"roc_auc\"] = float(roc_auc)\n\
          \n    # Save confusion matrix data\n    with open(confusion_matrix.path,\
          \ 'w') as f:\n        json.dump({\n            \"target_names\": [\"Negative\"\
          , \"Positive\"],\n            \"matrix\": conf_matrix\n        }, f)\n\n\
          \    # Add confusion matrix as metadata for the ClassificationMetrics artifact\n\
          \    confusion_matrix.metadata[\"format\"] = \"matrix\"\n    confusion_matrix.metadata[\"\
          labels\"] = [\"Negative\", \"Positive\"]\n    confusion_matrix.metadata[\"\
          matrix\"] = conf_matrix\n\n    # Generate HTML for plots\n    html_content\
          \ = f\"\"\"\n    <html>\n    <head>\n        <title>Model Evaluation: {model_name}</title>\n\
          \        <style>\n            body {{ font-family: Arial, sans-serif; margin:\
          \ 20px; }}\n            .metrics-container {{ display: flex; flex-wrap:\
          \ wrap; }}\n            .metric-box {{ \n                border: 1px solid\
          \ #ddd; \n                border-radius: 5px; \n                padding:\
          \ 15px; \n                margin: 10px; \n                background-color:\
          \ #f9f9f9; \n                width: 300px;\n            }}\n           \
          \ .metric-value {{ \n                font-size: 24px; \n               \
          \ font-weight: bold; \n                color: #2a5885; \n            }}\n\
          \            .plot-container {{ margin-top: 20px; }}\n            h2 {{\
          \ color: #444; }}\n        </style>\n    </head>\n    <body>\n        <h1>Evaluation\
          \ Results for {model_name}</h1>\n\n        <div class=\"metrics-container\"\
          >\n            <div class=\"metric-box\">\n                <h3>Accuracy</h3>\n\
          \                <div class=\"metric-value\">{accuracy:.4f}</div>\n    \
          \        </div>\n    \"\"\"\n\n    # Add other metrics\n    for metric_name,\
          \ metric_value in metrics_dict.items():\n        if metric_name != \"accuracy\"\
          :  # Already added above\n            html_content += f\"\"\"\n        \
          \    <div class=\"metric-box\">\n                <h3>{metric_name.replace('_',\
          \ ' ').title()}</h3>\n                <div class=\"metric-value\">{metric_value:.4f}</div>\n\
          \            </div>\n            \"\"\"\n\n    html_content += \"\"\"\n\
          \        </div>\n\n        <div class=\"plot-container\">\n            <h2>Confusion\
          \ Matrix</h2>\n            <img src=\"data:image/png;base64,{confusion_matrix_img}\"\
          \ alt=\"Confusion Matrix\">\n        </div>\n    \"\"\"\n\n    # Create\
          \ confusion matrix visualization\n    plt.figure(figsize=(8, 6))\n    plt.imshow(conf_matrix,\
          \ interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(f'Confusion\
          \ Matrix - {model_name}')\n    plt.colorbar()\n    classes = ['Negative',\
          \ 'Positive']\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks,\
          \ classes)\n    plt.yticks(tick_marks, classes)\n\n    # Add text annotations\
          \ to confusion matrix cells\n    thresh = np.array(conf_matrix).max() /\
          \ 2\n    for i in range(len(classes)):\n        for j in range(len(classes)):\n\
          \            plt.text(j, i, conf_matrix[i][j],\n                     horizontalalignment=\"\
          center\",\n                     color=\"white\" if conf_matrix[i][j] > thresh\
          \ else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n\
          \    plt.xlabel('Predicted label')\n\n    # Save the confusion matrix plot\n\
          \    buffer = BytesIO()\n    plt.savefig(buffer, format='png')\n    buffer.seek(0)\n\
          \    confusion_matrix_img = base64.b64encode(buffer.read()).decode('utf-8')\n\
          \    plt.close()\n\n    # Add to HTML\n    html_content = html_content.replace(\"\
          {confusion_matrix_img}\", confusion_matrix_img)\n\n    # Add ROC curve if\
          \ available\n    if y_pred_proba is not None:\n        plt.figure(figsize=(8,\
          \ 6))\n        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC\
          \ curve (area = {roc_auc:.2f})')\n        plt.plot([0, 1], [0, 1], color='navy',\
          \ lw=2, linestyle='--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0,\
          \ 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True\
          \ Positive Rate')\n        plt.title(f'ROC Curve - {model_name}')\n    \
          \    plt.legend(loc=\"lower right\")\n\n        buffer = BytesIO()\n   \
          \     plt.savefig(buffer, format='png')\n        buffer.seek(0)\n      \
          \  roc_image = base64.b64encode(buffer.read()).decode('utf-8')\n       \
          \ plt.close()\n\n        html_content += f\"\"\"\n        <div class=\"\
          plot-container\">\n            <h2>ROC Curve</h2>\n            <img src=\"\
          data:image/png;base64,{roc_image}\" alt=\"ROC Curve\">\n        </div>\n\
          \        \"\"\"\n\n    # For tree-based models, create feature importance\
          \ plot\n    if hasattr(model_obj, 'feature_importances_'):\n        importances\
          \ = model_obj.feature_importances_\n\n        plt.figure(figsize=(10, 6))\n\
          \        indices = np.argsort(importances)[::-1]\n        plt.bar(range(len(importances)),\
          \ importances[indices])\n        plt.title(f'Feature Importance - {model_name}')\n\
          \        plt.xticks(range(len(importances)), [feature_names_list[i] for\
          \ i in indices], rotation=90)\n        plt.tight_layout()\n\n        buffer\
          \ = BytesIO()\n        plt.savefig(buffer, format='png')\n        buffer.seek(0)\n\
          \        importance_image = base64.b64encode(buffer.read()).decode('utf-8')\n\
          \        plt.close()\n\n        html_content += f\"\"\"\n        <div class=\"\
          plot-container\">\n            <h2>Feature Importance</h2>\n           \
          \ <img src=\"data:image/png;base64,{importance_image}\" alt=\"Feature Importance\"\
          >\n        </div>\n        \"\"\"\n\n        # Add feature importances to\
          \ metrics\n        feature_imp_dict = dict(zip([feature_names_list[i] for\
          \ i in indices], importances[indices].tolist()))\n        for feature, importance\
          \ in feature_imp_dict.items():\n            metrics_dict[f\"importance_{feature}\"\
          ] = float(importance)\n\n    # Close HTML\n    html_content += \"\"\"\n\
          \    </body>\n    </html>\n    \"\"\"\n\n    # Write HTML to output\n  \
          \  with open(evaluation_plots.path, 'w') as f:\n        f.write(html_content)\n\
          \n    # Add metadata to metrics artifact\n    metrics.metadata.update({\n\
          \        'model_name': model_name,\n        'creation_time': str(datetime.datetime.now()),\n\
          \        'accuracy': float(accuracy),\n        'dataset_size': len(y_test)\n\
          \    })\n\n    # Add all metrics to the metrics artifact\n    for key, value\
          \ in metrics_dict.items():\n        metrics.metadata[key] = value\n\n"
        image: python:3.9
    exec-evaluate-model-2:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - evaluate_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'numpy' 'scikit-learn'\
          \ 'joblib' 'matplotlib' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef evaluate_model(model: Input[Model], \n                  test_data:\
          \ Input[Dataset], \n                  feature_names: Input[Dataset], \n\
          \                  model_name: str,\n                  metrics: Output[Metrics],\n\
          \                  confusion_matrix: Output[ClassificationMetrics],\n  \
          \                evaluation_plots: Output[HTML]):\n    import joblib\n \
          \   import numpy as np\n    import json\n    import os\n    import datetime\n\
          \    from sklearn.metrics import accuracy_score, classification_report,\
          \ confusion_matrix as cm\n    from sklearn.metrics import roc_curve, auc,\
          \ precision_recall_curve\n    import matplotlib.pyplot as plt\n    import\
          \ base64\n    from io import BytesIO\n\n    # Load model, test data, and\
          \ feature names\n    model_obj = joblib.load(model.path)\n    test_data_combined\
          \ = joblib.load(test_data.path)\n    feature_names_list = joblib.load(feature_names.path)\n\
          \n    # Extract features and target\n    X_test = test_data_combined[:,\
          \ :-1]\n    y_test = test_data_combined[:, -1]\n\n    # Make predictions\n\
          \    y_pred = model_obj.predict(X_test)\n    y_pred_proba = model_obj.predict_proba(X_test)[:,\
          \ 1] if hasattr(model_obj, \"predict_proba\") else None\n\n    # Calculate\
          \ metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    class_report\
          \ = classification_report(y_test, y_pred, output_dict=True)\n    conf_matrix\
          \ = cm(y_test, y_pred).tolist()\n\n    # Print the keys of class_report\
          \ to help debugging\n    print(\"Classification report keys:\", class_report.keys())\n\
          \n    # Store metrics - check if keys are strings or integers\n    metrics_dict\
          \ = {\n        \"accuracy\": float(accuracy)\n    }\n\n    # Handle different\
          \ possible key formats in classification report\n    if '0' in class_report:\n\
          \        # String keys\n        metrics_dict.update({\n            \"precision_class_0\"\
          : class_report['0']['precision'],\n            \"recall_class_0\": class_report['0']['recall'],\n\
          \            \"f1_score_class_0\": class_report['0']['f1-score'],\n    \
          \        \"precision_class_1\": class_report['1']['precision'],\n      \
          \      \"recall_class_1\": class_report['1']['recall'],\n            \"\
          f1_score_class_1\": class_report['1']['f1-score'],\n        })\n    elif\
          \ 0 in class_report:\n        # Integer keys\n        metrics_dict.update({\n\
          \            \"precision_class_0\": class_report[0]['precision'],\n    \
          \        \"recall_class_0\": class_report[0]['recall'],\n            \"\
          f1_score_class_0\": class_report[0]['f1-score'],\n            \"precision_class_1\"\
          : class_report[1]['precision'],\n            \"recall_class_1\": class_report[1]['recall'],\n\
          \            \"f1_score_class_1\": class_report[1]['f1-score'],\n      \
          \  })\n    else:\n        # If neither format works, try to find the actual\
          \ keys\n        # This handles cases where scikit-learn might label classes\
          \ differently\n        class_keys = [k for k in class_report.keys() if k\
          \ not in ['accuracy', 'macro avg', 'weighted avg']]\n        if len(class_keys)\
          \ >= 2:\n            metrics_dict.update({\n                \"precision_class_0\"\
          : class_report[class_keys[0]]['precision'],\n                \"recall_class_0\"\
          : class_report[class_keys[0]]['recall'],\n                \"f1_score_class_0\"\
          : class_report[class_keys[0]]['f1-score'],\n                \"precision_class_1\"\
          : class_report[class_keys[1]]['precision'],\n                \"recall_class_1\"\
          : class_report[class_keys[1]]['recall'],\n                \"f1_score_class_1\"\
          : class_report[class_keys[1]]['f1-score'],\n            })\n\n    # Calculate\
          \ ROC and AUC if probabilities are available\n    if y_pred_proba is not\
          \ None:\n        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n       \
          \ roc_auc = auc(fpr, tpr)\n        metrics_dict[\"roc_auc\"] = float(roc_auc)\n\
          \n    # Save confusion matrix data\n    with open(confusion_matrix.path,\
          \ 'w') as f:\n        json.dump({\n            \"target_names\": [\"Negative\"\
          , \"Positive\"],\n            \"matrix\": conf_matrix\n        }, f)\n\n\
          \    # Add confusion matrix as metadata for the ClassificationMetrics artifact\n\
          \    confusion_matrix.metadata[\"format\"] = \"matrix\"\n    confusion_matrix.metadata[\"\
          labels\"] = [\"Negative\", \"Positive\"]\n    confusion_matrix.metadata[\"\
          matrix\"] = conf_matrix\n\n    # Generate HTML for plots\n    html_content\
          \ = f\"\"\"\n    <html>\n    <head>\n        <title>Model Evaluation: {model_name}</title>\n\
          \        <style>\n            body {{ font-family: Arial, sans-serif; margin:\
          \ 20px; }}\n            .metrics-container {{ display: flex; flex-wrap:\
          \ wrap; }}\n            .metric-box {{ \n                border: 1px solid\
          \ #ddd; \n                border-radius: 5px; \n                padding:\
          \ 15px; \n                margin: 10px; \n                background-color:\
          \ #f9f9f9; \n                width: 300px;\n            }}\n           \
          \ .metric-value {{ \n                font-size: 24px; \n               \
          \ font-weight: bold; \n                color: #2a5885; \n            }}\n\
          \            .plot-container {{ margin-top: 20px; }}\n            h2 {{\
          \ color: #444; }}\n        </style>\n    </head>\n    <body>\n        <h1>Evaluation\
          \ Results for {model_name}</h1>\n\n        <div class=\"metrics-container\"\
          >\n            <div class=\"metric-box\">\n                <h3>Accuracy</h3>\n\
          \                <div class=\"metric-value\">{accuracy:.4f}</div>\n    \
          \        </div>\n    \"\"\"\n\n    # Add other metrics\n    for metric_name,\
          \ metric_value in metrics_dict.items():\n        if metric_name != \"accuracy\"\
          :  # Already added above\n            html_content += f\"\"\"\n        \
          \    <div class=\"metric-box\">\n                <h3>{metric_name.replace('_',\
          \ ' ').title()}</h3>\n                <div class=\"metric-value\">{metric_value:.4f}</div>\n\
          \            </div>\n            \"\"\"\n\n    html_content += \"\"\"\n\
          \        </div>\n\n        <div class=\"plot-container\">\n            <h2>Confusion\
          \ Matrix</h2>\n            <img src=\"data:image/png;base64,{confusion_matrix_img}\"\
          \ alt=\"Confusion Matrix\">\n        </div>\n    \"\"\"\n\n    # Create\
          \ confusion matrix visualization\n    plt.figure(figsize=(8, 6))\n    plt.imshow(conf_matrix,\
          \ interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(f'Confusion\
          \ Matrix - {model_name}')\n    plt.colorbar()\n    classes = ['Negative',\
          \ 'Positive']\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks,\
          \ classes)\n    plt.yticks(tick_marks, classes)\n\n    # Add text annotations\
          \ to confusion matrix cells\n    thresh = np.array(conf_matrix).max() /\
          \ 2\n    for i in range(len(classes)):\n        for j in range(len(classes)):\n\
          \            plt.text(j, i, conf_matrix[i][j],\n                     horizontalalignment=\"\
          center\",\n                     color=\"white\" if conf_matrix[i][j] > thresh\
          \ else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n\
          \    plt.xlabel('Predicted label')\n\n    # Save the confusion matrix plot\n\
          \    buffer = BytesIO()\n    plt.savefig(buffer, format='png')\n    buffer.seek(0)\n\
          \    confusion_matrix_img = base64.b64encode(buffer.read()).decode('utf-8')\n\
          \    plt.close()\n\n    # Add to HTML\n    html_content = html_content.replace(\"\
          {confusion_matrix_img}\", confusion_matrix_img)\n\n    # Add ROC curve if\
          \ available\n    if y_pred_proba is not None:\n        plt.figure(figsize=(8,\
          \ 6))\n        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC\
          \ curve (area = {roc_auc:.2f})')\n        plt.plot([0, 1], [0, 1], color='navy',\
          \ lw=2, linestyle='--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0,\
          \ 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True\
          \ Positive Rate')\n        plt.title(f'ROC Curve - {model_name}')\n    \
          \    plt.legend(loc=\"lower right\")\n\n        buffer = BytesIO()\n   \
          \     plt.savefig(buffer, format='png')\n        buffer.seek(0)\n      \
          \  roc_image = base64.b64encode(buffer.read()).decode('utf-8')\n       \
          \ plt.close()\n\n        html_content += f\"\"\"\n        <div class=\"\
          plot-container\">\n            <h2>ROC Curve</h2>\n            <img src=\"\
          data:image/png;base64,{roc_image}\" alt=\"ROC Curve\">\n        </div>\n\
          \        \"\"\"\n\n    # For tree-based models, create feature importance\
          \ plot\n    if hasattr(model_obj, 'feature_importances_'):\n        importances\
          \ = model_obj.feature_importances_\n\n        plt.figure(figsize=(10, 6))\n\
          \        indices = np.argsort(importances)[::-1]\n        plt.bar(range(len(importances)),\
          \ importances[indices])\n        plt.title(f'Feature Importance - {model_name}')\n\
          \        plt.xticks(range(len(importances)), [feature_names_list[i] for\
          \ i in indices], rotation=90)\n        plt.tight_layout()\n\n        buffer\
          \ = BytesIO()\n        plt.savefig(buffer, format='png')\n        buffer.seek(0)\n\
          \        importance_image = base64.b64encode(buffer.read()).decode('utf-8')\n\
          \        plt.close()\n\n        html_content += f\"\"\"\n        <div class=\"\
          plot-container\">\n            <h2>Feature Importance</h2>\n           \
          \ <img src=\"data:image/png;base64,{importance_image}\" alt=\"Feature Importance\"\
          >\n        </div>\n        \"\"\"\n\n        # Add feature importances to\
          \ metrics\n        feature_imp_dict = dict(zip([feature_names_list[i] for\
          \ i in indices], importances[indices].tolist()))\n        for feature, importance\
          \ in feature_imp_dict.items():\n            metrics_dict[f\"importance_{feature}\"\
          ] = float(importance)\n\n    # Close HTML\n    html_content += \"\"\"\n\
          \    </body>\n    </html>\n    \"\"\"\n\n    # Write HTML to output\n  \
          \  with open(evaluation_plots.path, 'w') as f:\n        f.write(html_content)\n\
          \n    # Add metadata to metrics artifact\n    metrics.metadata.update({\n\
          \        'model_name': model_name,\n        'creation_time': str(datetime.datetime.now()),\n\
          \        'accuracy': float(accuracy),\n        'dataset_size': len(y_test)\n\
          \    })\n\n    # Add all metrics to the metrics artifact\n    for key, value\
          \ in metrics_dict.items():\n        metrics.metadata[key] = value\n\n"
        image: python:3.9
    exec-evaluate-model-3:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - evaluate_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'numpy' 'scikit-learn'\
          \ 'joblib' 'matplotlib' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef evaluate_model(model: Input[Model], \n                  test_data:\
          \ Input[Dataset], \n                  feature_names: Input[Dataset], \n\
          \                  model_name: str,\n                  metrics: Output[Metrics],\n\
          \                  confusion_matrix: Output[ClassificationMetrics],\n  \
          \                evaluation_plots: Output[HTML]):\n    import joblib\n \
          \   import numpy as np\n    import json\n    import os\n    import datetime\n\
          \    from sklearn.metrics import accuracy_score, classification_report,\
          \ confusion_matrix as cm\n    from sklearn.metrics import roc_curve, auc,\
          \ precision_recall_curve\n    import matplotlib.pyplot as plt\n    import\
          \ base64\n    from io import BytesIO\n\n    # Load model, test data, and\
          \ feature names\n    model_obj = joblib.load(model.path)\n    test_data_combined\
          \ = joblib.load(test_data.path)\n    feature_names_list = joblib.load(feature_names.path)\n\
          \n    # Extract features and target\n    X_test = test_data_combined[:,\
          \ :-1]\n    y_test = test_data_combined[:, -1]\n\n    # Make predictions\n\
          \    y_pred = model_obj.predict(X_test)\n    y_pred_proba = model_obj.predict_proba(X_test)[:,\
          \ 1] if hasattr(model_obj, \"predict_proba\") else None\n\n    # Calculate\
          \ metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    class_report\
          \ = classification_report(y_test, y_pred, output_dict=True)\n    conf_matrix\
          \ = cm(y_test, y_pred).tolist()\n\n    # Print the keys of class_report\
          \ to help debugging\n    print(\"Classification report keys:\", class_report.keys())\n\
          \n    # Store metrics - check if keys are strings or integers\n    metrics_dict\
          \ = {\n        \"accuracy\": float(accuracy)\n    }\n\n    # Handle different\
          \ possible key formats in classification report\n    if '0' in class_report:\n\
          \        # String keys\n        metrics_dict.update({\n            \"precision_class_0\"\
          : class_report['0']['precision'],\n            \"recall_class_0\": class_report['0']['recall'],\n\
          \            \"f1_score_class_0\": class_report['0']['f1-score'],\n    \
          \        \"precision_class_1\": class_report['1']['precision'],\n      \
          \      \"recall_class_1\": class_report['1']['recall'],\n            \"\
          f1_score_class_1\": class_report['1']['f1-score'],\n        })\n    elif\
          \ 0 in class_report:\n        # Integer keys\n        metrics_dict.update({\n\
          \            \"precision_class_0\": class_report[0]['precision'],\n    \
          \        \"recall_class_0\": class_report[0]['recall'],\n            \"\
          f1_score_class_0\": class_report[0]['f1-score'],\n            \"precision_class_1\"\
          : class_report[1]['precision'],\n            \"recall_class_1\": class_report[1]['recall'],\n\
          \            \"f1_score_class_1\": class_report[1]['f1-score'],\n      \
          \  })\n    else:\n        # If neither format works, try to find the actual\
          \ keys\n        # This handles cases where scikit-learn might label classes\
          \ differently\n        class_keys = [k for k in class_report.keys() if k\
          \ not in ['accuracy', 'macro avg', 'weighted avg']]\n        if len(class_keys)\
          \ >= 2:\n            metrics_dict.update({\n                \"precision_class_0\"\
          : class_report[class_keys[0]]['precision'],\n                \"recall_class_0\"\
          : class_report[class_keys[0]]['recall'],\n                \"f1_score_class_0\"\
          : class_report[class_keys[0]]['f1-score'],\n                \"precision_class_1\"\
          : class_report[class_keys[1]]['precision'],\n                \"recall_class_1\"\
          : class_report[class_keys[1]]['recall'],\n                \"f1_score_class_1\"\
          : class_report[class_keys[1]]['f1-score'],\n            })\n\n    # Calculate\
          \ ROC and AUC if probabilities are available\n    if y_pred_proba is not\
          \ None:\n        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n       \
          \ roc_auc = auc(fpr, tpr)\n        metrics_dict[\"roc_auc\"] = float(roc_auc)\n\
          \n    # Save confusion matrix data\n    with open(confusion_matrix.path,\
          \ 'w') as f:\n        json.dump({\n            \"target_names\": [\"Negative\"\
          , \"Positive\"],\n            \"matrix\": conf_matrix\n        }, f)\n\n\
          \    # Add confusion matrix as metadata for the ClassificationMetrics artifact\n\
          \    confusion_matrix.metadata[\"format\"] = \"matrix\"\n    confusion_matrix.metadata[\"\
          labels\"] = [\"Negative\", \"Positive\"]\n    confusion_matrix.metadata[\"\
          matrix\"] = conf_matrix\n\n    # Generate HTML for plots\n    html_content\
          \ = f\"\"\"\n    <html>\n    <head>\n        <title>Model Evaluation: {model_name}</title>\n\
          \        <style>\n            body {{ font-family: Arial, sans-serif; margin:\
          \ 20px; }}\n            .metrics-container {{ display: flex; flex-wrap:\
          \ wrap; }}\n            .metric-box {{ \n                border: 1px solid\
          \ #ddd; \n                border-radius: 5px; \n                padding:\
          \ 15px; \n                margin: 10px; \n                background-color:\
          \ #f9f9f9; \n                width: 300px;\n            }}\n           \
          \ .metric-value {{ \n                font-size: 24px; \n               \
          \ font-weight: bold; \n                color: #2a5885; \n            }}\n\
          \            .plot-container {{ margin-top: 20px; }}\n            h2 {{\
          \ color: #444; }}\n        </style>\n    </head>\n    <body>\n        <h1>Evaluation\
          \ Results for {model_name}</h1>\n\n        <div class=\"metrics-container\"\
          >\n            <div class=\"metric-box\">\n                <h3>Accuracy</h3>\n\
          \                <div class=\"metric-value\">{accuracy:.4f}</div>\n    \
          \        </div>\n    \"\"\"\n\n    # Add other metrics\n    for metric_name,\
          \ metric_value in metrics_dict.items():\n        if metric_name != \"accuracy\"\
          :  # Already added above\n            html_content += f\"\"\"\n        \
          \    <div class=\"metric-box\">\n                <h3>{metric_name.replace('_',\
          \ ' ').title()}</h3>\n                <div class=\"metric-value\">{metric_value:.4f}</div>\n\
          \            </div>\n            \"\"\"\n\n    html_content += \"\"\"\n\
          \        </div>\n\n        <div class=\"plot-container\">\n            <h2>Confusion\
          \ Matrix</h2>\n            <img src=\"data:image/png;base64,{confusion_matrix_img}\"\
          \ alt=\"Confusion Matrix\">\n        </div>\n    \"\"\"\n\n    # Create\
          \ confusion matrix visualization\n    plt.figure(figsize=(8, 6))\n    plt.imshow(conf_matrix,\
          \ interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(f'Confusion\
          \ Matrix - {model_name}')\n    plt.colorbar()\n    classes = ['Negative',\
          \ 'Positive']\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks,\
          \ classes)\n    plt.yticks(tick_marks, classes)\n\n    # Add text annotations\
          \ to confusion matrix cells\n    thresh = np.array(conf_matrix).max() /\
          \ 2\n    for i in range(len(classes)):\n        for j in range(len(classes)):\n\
          \            plt.text(j, i, conf_matrix[i][j],\n                     horizontalalignment=\"\
          center\",\n                     color=\"white\" if conf_matrix[i][j] > thresh\
          \ else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n\
          \    plt.xlabel('Predicted label')\n\n    # Save the confusion matrix plot\n\
          \    buffer = BytesIO()\n    plt.savefig(buffer, format='png')\n    buffer.seek(0)\n\
          \    confusion_matrix_img = base64.b64encode(buffer.read()).decode('utf-8')\n\
          \    plt.close()\n\n    # Add to HTML\n    html_content = html_content.replace(\"\
          {confusion_matrix_img}\", confusion_matrix_img)\n\n    # Add ROC curve if\
          \ available\n    if y_pred_proba is not None:\n        plt.figure(figsize=(8,\
          \ 6))\n        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC\
          \ curve (area = {roc_auc:.2f})')\n        plt.plot([0, 1], [0, 1], color='navy',\
          \ lw=2, linestyle='--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0,\
          \ 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True\
          \ Positive Rate')\n        plt.title(f'ROC Curve - {model_name}')\n    \
          \    plt.legend(loc=\"lower right\")\n\n        buffer = BytesIO()\n   \
          \     plt.savefig(buffer, format='png')\n        buffer.seek(0)\n      \
          \  roc_image = base64.b64encode(buffer.read()).decode('utf-8')\n       \
          \ plt.close()\n\n        html_content += f\"\"\"\n        <div class=\"\
          plot-container\">\n            <h2>ROC Curve</h2>\n            <img src=\"\
          data:image/png;base64,{roc_image}\" alt=\"ROC Curve\">\n        </div>\n\
          \        \"\"\"\n\n    # For tree-based models, create feature importance\
          \ plot\n    if hasattr(model_obj, 'feature_importances_'):\n        importances\
          \ = model_obj.feature_importances_\n\n        plt.figure(figsize=(10, 6))\n\
          \        indices = np.argsort(importances)[::-1]\n        plt.bar(range(len(importances)),\
          \ importances[indices])\n        plt.title(f'Feature Importance - {model_name}')\n\
          \        plt.xticks(range(len(importances)), [feature_names_list[i] for\
          \ i in indices], rotation=90)\n        plt.tight_layout()\n\n        buffer\
          \ = BytesIO()\n        plt.savefig(buffer, format='png')\n        buffer.seek(0)\n\
          \        importance_image = base64.b64encode(buffer.read()).decode('utf-8')\n\
          \        plt.close()\n\n        html_content += f\"\"\"\n        <div class=\"\
          plot-container\">\n            <h2>Feature Importance</h2>\n           \
          \ <img src=\"data:image/png;base64,{importance_image}\" alt=\"Feature Importance\"\
          >\n        </div>\n        \"\"\"\n\n        # Add feature importances to\
          \ metrics\n        feature_imp_dict = dict(zip([feature_names_list[i] for\
          \ i in indices], importances[indices].tolist()))\n        for feature, importance\
          \ in feature_imp_dict.items():\n            metrics_dict[f\"importance_{feature}\"\
          ] = float(importance)\n\n    # Close HTML\n    html_content += \"\"\"\n\
          \    </body>\n    </html>\n    \"\"\"\n\n    # Write HTML to output\n  \
          \  with open(evaluation_plots.path, 'w') as f:\n        f.write(html_content)\n\
          \n    # Add metadata to metrics artifact\n    metrics.metadata.update({\n\
          \        'model_name': model_name,\n        'creation_time': str(datetime.datetime.now()),\n\
          \        'accuracy': float(accuracy),\n        'dataset_size': len(y_test)\n\
          \    })\n\n    # Add all metrics to the metrics artifact\n    for key, value\
          \ in metrics_dict.items():\n        metrics.metadata[key] = value\n\n"
        image: python:3.9
    exec-load-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - load_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'numpy' 'scikit-learn'\
          \ 'joblib' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef load_data(dataset: Output[Dataset], \n              feature_names:\
          \ Output[Dataset], \n              metadata: Output[Dataset]):\n    import\
          \ numpy as np\n    from sklearn.datasets import load_diabetes\n    import\
          \ joblib\n    import json\n    import os\n    import datetime\n\n    # Load\
          \ diabetes dataset\n    diabetes = load_diabetes()\n    X = diabetes.data\n\
          \n    # Convert to binary classification (above/below median)\n    y = (diabetes.target\
          \ > np.median(diabetes.target)).astype(int)\n\n    # Create a combined dataset\
          \ with features and target\n    combined_data = np.column_stack((X, y))\n\
          \n    # Save data to local path - the artifact.path gives us the local path\n\
          \    os.makedirs(os.path.dirname(dataset.path), exist_ok=True)\n    joblib.dump(combined_data,\
          \ dataset.path)\n\n    # Save feature names for later use\n    feature_names_list\
          \ = list(diabetes.feature_names)\n    joblib.dump(feature_names_list, feature_names.path)\n\
          \n    # Generate dataset metadata\n    metadata_dict = {\n        \"num_samples\"\
          : X.shape[0],\n        \"num_features\": X.shape[1],\n        \"class_distribution\"\
          : np.bincount(y).tolist(),\n        \"feature_names\": feature_names_list\n\
          \    }\n\n    with open(metadata.path, 'w') as f:\n        json.dump(metadata_dict,\
          \ f)\n\n    # Add metadata to the dataset artifact\n    dataset.metadata.update({\n\
          \        'creation_time': str(datetime.datetime.now()),\n        'format':\
          \ 'joblib',\n        'size': X.shape[0],\n        'sample_count': X.shape[0],\n\
          \        'description': 'Diabetes dataset converted to binary classification',\n\
          \        'source': 'sklearn.datasets',\n        'feature_count': X.shape[1],\n\
          \        'positive_class_samples': int(np.sum(y)),\n        'negative_class_samples':\
          \ int(np.sum(1-y))\n    })\n\n    feature_names.metadata.update({\n    \
          \    'creation_time': str(datetime.datetime.now()),\n        'description':\
          \ 'Feature names for the diabetes dataset',\n    })\n\n    metadata.metadata.update({\n\
          \        'creation_time': str(datetime.datetime.now()),\n        'description':\
          \ 'Metadata about the diabetes dataset'\n    })\n\n"
        image: python:3.9
    exec-preprocess-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - preprocess_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'scikit-learn'\
          \ 'joblib' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef preprocess_data(train_data: Input[Dataset], \n              \
          \     test_data: Input[Dataset],\n                   processed_train_data:\
          \ Output[Dataset],\n                   processed_test_data: Output[Dataset],\n\
          \                   preprocessor: Output[Model]):\n    import joblib\n \
          \   import numpy as np\n    import datetime\n    import json\n    from sklearn.preprocessing\
          \ import StandardScaler\n\n    # Load training and test data\n    train_data_combined\
          \ = joblib.load(train_data.path)\n    test_data_combined = joblib.load(test_data.path)\n\
          \n    # Extract features and targets\n    X_train = train_data_combined[:,\
          \ :-1]\n    y_train = train_data_combined[:, -1]\n    X_test = test_data_combined[:,\
          \ :-1]\n    y_test = test_data_combined[:, -1]\n\n    # Initialize and fit\
          \ the scaler\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n\
          \    X_test_scaled = scaler.transform(X_test)\n\n    # Recombine with targets\n\
          \    train_data_scaled = np.column_stack((X_train_scaled, y_train))\n  \
          \  test_data_scaled = np.column_stack((X_test_scaled, y_test))\n\n    #\
          \ Save the preprocessed data and the scaler\n    joblib.dump(train_data_scaled,\
          \ processed_train_data.path)\n    joblib.dump(test_data_scaled, processed_test_data.path)\n\
          \    joblib.dump(scaler, preprocessor.path)\n\n    # Add metadata\n    preprocessor.metadata.update({\n\
          \        'creation_time': str(datetime.datetime.now()),\n        'description':\
          \ 'StandardScaler for feature normalization',\n        'framework': 'scikit-learn',\n\
          \        'type': 'StandardScaler'\n    })\n\n    processed_train_data.metadata.update({\n\
          \        'creation_time': str(datetime.datetime.now()),\n        'description':\
          \ 'Preprocessed training data',\n        'preprocessing': 'StandardScaler',\n\
          \        'samples': train_data_combined.shape[0]\n    })\n\n    processed_test_data.metadata.update({\n\
          \        'creation_time': str(datetime.datetime.now()),\n        'description':\
          \ 'Preprocessed test data',\n        'preprocessing': 'StandardScaler',\n\
          \        'samples': test_data_combined.shape[0]\n    })\n\n"
        image: python:3.9
    exec-split-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - split_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'numpy' 'scikit-learn'\
          \ 'joblib' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef split_data(dataset: Input[Dataset], \n               train_data:\
          \ Output[Dataset],\n               test_data: Output[Dataset],\n       \
          \        split_info: Output[Dataset],\n               test_size: float =\
          \ 0.3, \n               random_state: int = 42):\n    import joblib\n  \
          \  import numpy as np\n    import json\n    import os\n    import datetime\n\
          \    from sklearn.model_selection import train_test_split\n\n    # Load\
          \ the data\n    combined_data = joblib.load(dataset.path)\n    X = combined_data[:,\
          \ :-1]  # All columns except the last one\n    y = combined_data[:, -1]\
          \   # Last column is the target\n\n    # Split the data\n    X_train, X_test,\
          \ y_train, y_test = train_test_split(\n        X, y, test_size=test_size,\
          \ random_state=random_state, stratify=y\n    )\n\n    # Create combined\
          \ datasets again\n    train_data_combined = np.column_stack((X_train, y_train))\n\
          \    test_data_combined = np.column_stack((X_test, y_test))\n\n    # Save\
          \ the splits\n    joblib.dump(train_data_combined, train_data.path)\n  \
          \  joblib.dump(test_data_combined, test_data.path)\n\n    # Convert numpy\
          \ types to native Python types for JSON serialization\n    train_unique,\
          \ train_counts = np.unique(y_train, return_counts=True)\n    test_unique,\
          \ test_counts = np.unique(y_test, return_counts=True)\n\n    # Convert numpy\
          \ arrays to regular Python lists/types\n    train_class_distribution = {int(k):\
          \ int(v) for k, v in zip(train_unique, train_counts)}\n    test_class_distribution\
          \ = {int(k): int(v) for k, v in zip(test_unique, test_counts)}\n\n    split_info_dict\
          \ = {\n        \"train_samples\": int(X_train.shape[0]),\n        \"test_samples\"\
          : int(X_test.shape[0]),\n        \"train_class_distribution\": train_class_distribution,\n\
          \        \"test_class_distribution\": test_class_distribution\n    }\n\n\
          \    with open(split_info.path, 'w') as f:\n        json.dump(split_info_dict,\
          \ f)\n\n    # Add metadata to artifacts\n    train_data.metadata.update({\n\
          \        'creation_time': str(datetime.datetime.now()),\n        'description':\
          \ 'Training data split',\n        'samples': int(X_train.shape[0]),\n  \
          \      'features': int(X_train.shape[1]),\n        'positive_samples': int(np.sum(y_train)),\n\
          \        'negative_samples': int(np.sum(y_train == 0)),\n        'split_ratio':\
          \ f'{1-test_size:.2f}/{test_size:.2f}'\n    })\n\n    test_data.metadata.update({\n\
          \        'creation_time': str(datetime.datetime.now()),\n        'description':\
          \ 'Test data split',\n        'samples': int(X_test.shape[0]),\n       \
          \ 'features': int(X_test.shape[1]),\n        'positive_samples': int(np.sum(y_test)),\n\
          \        'negative_samples': int(np.sum(y_test == 0)),\n        'split_ratio':\
          \ f'{1-test_size:.2f}/{test_size:.2f}'\n    })\n\n    split_info.metadata.update({\n\
          \        'creation_time': str(datetime.datetime.now()),\n        'description':\
          \ 'Information about the train/test split',\n        'test_size': test_size,\n\
          \        'random_state': random_state\n    })\n\n"
        image: python:3.9
    exec-train-decision-tree:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_decision_tree
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'scikit-learn'\
          \ 'joblib' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_decision_tree(train_data: Input[Dataset], \n          \
          \              model: Output[Model],\n                        max_depth:\
          \ int = 5, \n                        random_state: int = 42):\n    import\
          \ joblib\n    import numpy as np\n    import datetime\n    import json\n\
          \    from sklearn.tree import DecisionTreeClassifier\n\n    # Load training\
          \ data\n    train_data_combined = joblib.load(train_data.path)\n    X_train\
          \ = train_data_combined[:, :-1]\n    y_train = train_data_combined[:, -1]\n\
          \n    # Train the model\n    dt_model = DecisionTreeClassifier(max_depth=max_depth,\
          \ random_state=random_state)\n    dt_model.fit(X_train, y_train)\n\n   \
          \ # Save the model\n    joblib.dump(dt_model, model.path)\n\n    # Add metadata\n\
          \    model.metadata.update({\n        'framework': 'scikit-learn',\n   \
          \     'model_type': 'DecisionTreeClassifier',\n        'creation_time':\
          \ str(datetime.datetime.now()),\n        'version': '1.0',\n        'hyperparameters':\
          \ json.dumps({\n            'max_depth': max_depth,\n            'random_state':\
          \ random_state\n        }),\n        'training_dataset_size': X_train.shape[0],\n\
          \        'feature_count': X_train.shape[1],\n        'description': 'Decision\
          \ Tree classifier for diabetes prediction'\n    })\n\n"
        image: python:3.9
    exec-train-random-forest:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_random_forest
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'scikit-learn'\
          \ 'joblib' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_random_forest(train_data: Input[Dataset], \n          \
          \             model: Output[Model],\n                       n_estimators:\
          \ int = 100, \n                       random_state: int = 42):\n    import\
          \ joblib\n    import numpy as np\n    import json\n    import datetime\n\
          \    from sklearn.ensemble import RandomForestClassifier\n\n    # Load training\
          \ data\n    train_data_combined = joblib.load(train_data.path)\n    X_train\
          \ = train_data_combined[:, :-1]\n    y_train = train_data_combined[:, -1]\n\
          \n    # Train the model\n    rf_model = RandomForestClassifier(n_estimators=n_estimators,\
          \ random_state=random_state)\n    rf_model.fit(X_train, y_train)\n\n   \
          \ # Save the model\n    joblib.dump(rf_model, model.path)\n\n    # Add metadata\n\
          \    model.metadata.update({\n        'framework': 'scikit-learn',\n   \
          \     'model_type': 'RandomForestClassifier',\n        'creation_time':\
          \ str(datetime.datetime.now()),\n        'version': '1.0',\n        'hyperparameters':\
          \ json.dumps({\n            'n_estimators': n_estimators,\n            'random_state':\
          \ random_state\n        }),\n        'training_dataset_size': X_train.shape[0],\n\
          \        'feature_count': X_train.shape[1],\n        'description': 'Random\
          \ Forest classifier for diabetes prediction'\n    })\n\n"
        image: python:3.9
    exec-train-svm:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_svm
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'scikit-learn'\
          \ 'joblib' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_svm(train_data: Input[Dataset], \n             model: Output[Model],\n\
          \             C: float = 1.0, \n             kernel: str = 'rbf', \n   \
          \          random_state: int = 42):\n    import joblib\n    import numpy\
          \ as np\n    import json\n    import datetime\n    from sklearn.svm import\
          \ SVC\n\n    # Load training data\n    train_data_combined = joblib.load(train_data.path)\n\
          \    X_train = train_data_combined[:, :-1]\n    y_train = train_data_combined[:,\
          \ -1]\n\n    # Train the model\n    svm_model = SVC(C=C, kernel=kernel,\
          \ random_state=random_state, probability=True)\n    svm_model.fit(X_train,\
          \ y_train)\n\n    # Save the model\n    joblib.dump(svm_model, model.path)\n\
          \n    # Add metadata\n    model.metadata.update({\n        'framework':\
          \ 'scikit-learn',\n        'model_type': 'SVC',\n        'creation_time':\
          \ str(datetime.datetime.now()),\n        'version': '1.0',\n        'hyperparameters':\
          \ json.dumps({\n            'C': C,\n            'kernel': kernel,\n   \
          \         'random_state': random_state,\n            'probability': True\n\
          \        }),\n        'training_dataset_size': X_train.shape[0],\n     \
          \   'feature_count': X_train.shape[1],\n        'description': 'Support\
          \ Vector Machine classifier for diabetes prediction'\n    })\n\n"
        image: python:3.9
pipelineInfo:
  description: A demonstration pipeline for diabetes classification using multiple
    models with artifact tracking
  name: diabetes-classification-pipeline
root:
  dag:
    tasks:
      compare-models:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-compare-models
        dependentTasks:
        - evaluate-model
        - evaluate-model-2
        - evaluate-model-3
        inputs:
          artifacts:
            dt_metrics:
              taskOutputArtifact:
                outputArtifactKey: metrics
                producerTask: evaluate-model
            rf_metrics:
              taskOutputArtifact:
                outputArtifactKey: metrics
                producerTask: evaluate-model-2
            svm_metrics:
              taskOutputArtifact:
                outputArtifactKey: metrics
                producerTask: evaluate-model-3
        taskInfo:
          name: compare-models
      evaluate-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-evaluate-model
        dependentTasks:
        - load-data
        - preprocess-data
        - train-decision-tree
        inputs:
          artifacts:
            feature_names:
              taskOutputArtifact:
                outputArtifactKey: feature_names
                producerTask: load-data
            model:
              taskOutputArtifact:
                outputArtifactKey: model
                producerTask: train-decision-tree
            test_data:
              taskOutputArtifact:
                outputArtifactKey: processed_test_data
                producerTask: preprocess-data
          parameters:
            model_name:
              runtimeValue:
                constant: Decision Tree
        taskInfo:
          name: evaluate-model
      evaluate-model-2:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-evaluate-model-2
        dependentTasks:
        - load-data
        - preprocess-data
        - train-random-forest
        inputs:
          artifacts:
            feature_names:
              taskOutputArtifact:
                outputArtifactKey: feature_names
                producerTask: load-data
            model:
              taskOutputArtifact:
                outputArtifactKey: model
                producerTask: train-random-forest
            test_data:
              taskOutputArtifact:
                outputArtifactKey: processed_test_data
                producerTask: preprocess-data
          parameters:
            model_name:
              runtimeValue:
                constant: Random Forest
        taskInfo:
          name: evaluate-model-2
      evaluate-model-3:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-evaluate-model-3
        dependentTasks:
        - load-data
        - preprocess-data
        - train-svm
        inputs:
          artifacts:
            feature_names:
              taskOutputArtifact:
                outputArtifactKey: feature_names
                producerTask: load-data
            model:
              taskOutputArtifact:
                outputArtifactKey: model
                producerTask: train-svm
            test_data:
              taskOutputArtifact:
                outputArtifactKey: processed_test_data
                producerTask: preprocess-data
          parameters:
            model_name:
              runtimeValue:
                constant: Support Vector Machine
        taskInfo:
          name: evaluate-model-3
      load-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-load-data
        taskInfo:
          name: load-data
      preprocess-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-preprocess-data
        dependentTasks:
        - split-data
        inputs:
          artifacts:
            test_data:
              taskOutputArtifact:
                outputArtifactKey: test_data
                producerTask: split-data
            train_data:
              taskOutputArtifact:
                outputArtifactKey: train_data
                producerTask: split-data
        taskInfo:
          name: preprocess-data
      split-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-split-data
        dependentTasks:
        - load-data
        inputs:
          artifacts:
            dataset:
              taskOutputArtifact:
                outputArtifactKey: dataset
                producerTask: load-data
          parameters:
            random_state:
              componentInputParameter: random_state
            test_size:
              componentInputParameter: test_size
        taskInfo:
          name: split-data
      train-decision-tree:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-decision-tree
        dependentTasks:
        - preprocess-data
        inputs:
          artifacts:
            train_data:
              taskOutputArtifact:
                outputArtifactKey: processed_train_data
                producerTask: preprocess-data
          parameters:
            max_depth:
              componentInputParameter: dt_max_depth
            random_state:
              componentInputParameter: random_state
        taskInfo:
          name: train-decision-tree
      train-random-forest:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-random-forest
        dependentTasks:
        - preprocess-data
        inputs:
          artifacts:
            train_data:
              taskOutputArtifact:
                outputArtifactKey: processed_train_data
                producerTask: preprocess-data
          parameters:
            n_estimators:
              componentInputParameter: rf_n_estimators
            random_state:
              componentInputParameter: random_state
        taskInfo:
          name: train-random-forest
      train-svm:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-svm
        dependentTasks:
        - preprocess-data
        inputs:
          artifacts:
            train_data:
              taskOutputArtifact:
                outputArtifactKey: processed_train_data
                producerTask: preprocess-data
          parameters:
            C:
              componentInputParameter: svm_c
            kernel:
              componentInputParameter: svm_kernel
            random_state:
              componentInputParameter: random_state
        taskInfo:
          name: train-svm
  inputDefinitions:
    parameters:
      dt_max_depth:
        defaultValue: 5.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      random_state:
        defaultValue: 42.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      rf_n_estimators:
        defaultValue: 100.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      svm_c:
        defaultValue: 1.0
        isOptional: true
        parameterType: NUMBER_DOUBLE
      svm_kernel:
        defaultValue: rbf
        isOptional: true
        parameterType: STRING
      test_size:
        defaultValue: 0.3
        isOptional: true
        parameterType: NUMBER_DOUBLE
schemaVersion: 2.1.0
sdkVersion: kfp-2.11.0
